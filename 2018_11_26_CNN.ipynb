{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2018_11_26_CNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YonghwanYim/Agent_Based_Model_Transport_System/blob/master/2018_11_26_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "yhdlHDjPlSu9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Google Drive 연동, 파일 생성**"
      ]
    },
    {
      "metadata": {
        "id": "Zj93Rvb3lFLn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive # Google Drive 인증\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eegK0t37ldOV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "a69e5eca-489e-4edd-bcd7-6e6237c7a068"
      },
      "cell_type": "code",
      "source": [
        "# mnist.py\n",
        "temp = drive.CreateFile({'id' : '1hPFSjrJOnbuWtSZguJy-tNgsq0nlGs64'}) # Generate File\n",
        "temp.GetContentFile('mnist.py') # Rename\n",
        "# functions.py\n",
        "temp = drive.CreateFile({'id' : '1QMzKrvnRBH-AbvnUJhB4MJYldhzUt2tY'}) # Generate File\n",
        "temp.GetContentFile('functions.py') # Rename\n",
        "# gradient.py\n",
        "temp = drive.CreateFile({'id' : '1-fV8DtnDHd-evmHGDOBNiNZWxW-YK4v1'}) # Generate File\n",
        "temp.GetContentFile('gradient.py') # Rename\n",
        "# layers.py\n",
        "temp = drive.CreateFile({'id' : '1tEvPr0PVrWF2wOrfWyNEKstNibn5LJMd'}) # Generate File\n",
        "temp.GetContentFile('layers.py') # Rename\n",
        "# multilayernet.py\n",
        "temp = drive.CreateFile({'id' : '1Ft_iIP9WVejT5YgbQnX6lkudpY-XHKQZ'}) # Generate File\n",
        "temp.GetContentFile('multilayernet.py') # Rename\n",
        "# multilayernetextend.py\n",
        "temp = drive.CreateFile({'id' : '1-fM0bu1aAPtx0se-biRJzSv-v9WgahDh'}) # Generate File\n",
        "temp.GetContentFile('multilayernetextend.py') # Rename\n",
        "# optimizer.py\n",
        "temp = drive.CreateFile({'id' : '1EGIbur6m0_mklzqKV-tiszOupE_SrCMX'}) # Generate File\n",
        "temp.GetContentFile('optimizer.py') # Rename\n",
        "# trainer.py\n",
        "temp = drive.CreateFile({'id' : '1PPDabIzpLbmLHBzQ1suW006hTyOxU-Bi'}) # Generate File\n",
        "temp.GetContentFile('trainer.py') # Rename\n",
        "# util.py\n",
        "temp = drive.CreateFile({'id' : '1t-3l-Kj2FAIHaH0yCGnhXx5tZnZ_Hy8H'}) # Generate File\n",
        "temp.GetContentFile('util.py') # Rename\n",
        "\n",
        "!ls # 생성된 파일 확인"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "adc.json      layers.py\t\t      multilayernet.py\ttrainer.py\n",
            "functions.py  mnist.py\t\t      optimizer.py\tutil.py\n",
            "gradient.py   multilayernetextend.py  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IPlr9esYln3F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Convolutional Neural Network**"
      ]
    },
    {
      "metadata": {
        "id": "oYGnPQDFl1IF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33197
        },
        "outputId": "8cf08489-3174-4688-c4c0-03c2d9206e6e"
      },
      "cell_type": "code",
      "source": [
        "# coding: utf-8\n",
        "import sys, os\n",
        "sys.path.append(os.pardir)  \n",
        "import pickle\n",
        "import numpy as np\n",
        "from collections import OrderedDict\n",
        "from layers import *\n",
        "from gradient import numerical_gradient\n",
        "import matplotlib.pyplot as plt\n",
        "from mnist import load_mnist\n",
        "from trainer import Trainer\n",
        "\n",
        "\n",
        "class SimpleConvNet:\n",
        "   \n",
        "    def __init__(self, input_dim=(1, 28, 28), \n",
        "                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
        "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
        "        filter_num = conv_param['filter_num']\n",
        "        filter_size = conv_param['filter_size']\n",
        "        filter_pad = conv_param['pad']\n",
        "        filter_stride = conv_param['stride']\n",
        "        input_size = input_dim[1]\n",
        "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
        "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
        "\n",
        "        \n",
        "        self.params = {}\n",
        "        self.params['W1'] = weight_init_std * \\\n",
        "                            np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
        "        self.params['b1'] = np.zeros(filter_num)\n",
        "        self.params['W2'] = weight_init_std * \\\n",
        "                            np.random.randn(pool_output_size, hidden_size)\n",
        "        self.params['b2'] = np.zeros(hidden_size)\n",
        "        self.params['W3'] = weight_init_std * \\\n",
        "                            np.random.randn(hidden_size, output_size)\n",
        "        self.params['b3'] = np.zeros(output_size)\n",
        "\n",
        "        \n",
        "        self.layers = OrderedDict()\n",
        "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n",
        "                                           conv_param['stride'], conv_param['pad'])\n",
        "        self.layers['Relu1'] = Relu()\n",
        "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
        "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
        "        self.layers['Relu2'] = Relu()\n",
        "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
        "\n",
        "        self.last_layer = SoftmaxWithLoss()\n",
        "\n",
        "    def predict(self, x):\n",
        "        for layer in self.layers.values():\n",
        "            x = layer.forward(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def loss(self, x, t):\n",
        "        \n",
        "        y = self.predict(x)\n",
        "        return self.last_layer.forward(y, t)\n",
        "\n",
        "    def accuracy(self, x, t, batch_size=100):\n",
        "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
        "        \n",
        "        acc = 0.0\n",
        "        \n",
        "        for i in range(int(x.shape[0] / batch_size)):\n",
        "            tx = x[i*batch_size:(i+1)*batch_size]\n",
        "            tt = t[i*batch_size:(i+1)*batch_size]\n",
        "            y = self.predict(tx)\n",
        "            y = np.argmax(y, axis=1)\n",
        "            acc += np.sum(y == tt) \n",
        "        \n",
        "        return acc / x.shape[0]\n",
        "\n",
        "    def numerical_gradient(self, x, t):\n",
        "        \n",
        "        loss_w = lambda w: self.loss(x, t)\n",
        "\n",
        "        grads = {}\n",
        "        for idx in (1, 2, 3):\n",
        "            grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])\n",
        "            grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])\n",
        "\n",
        "        return grads\n",
        "\n",
        "    def gradient(self, x, t):\n",
        "        \n",
        "        # forward\n",
        "        self.loss(x, t)\n",
        "\n",
        "        # backward\n",
        "        dout = 1\n",
        "        dout = self.last_layer.backward(dout)\n",
        "\n",
        "        layers = list(self.layers.values())\n",
        "        layers.reverse()\n",
        "        for layer in layers:\n",
        "            dout = layer.backward(dout)\n",
        "\n",
        "        \n",
        "        grads = {}\n",
        "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
        "        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
        "        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
        "\n",
        "        return grads\n",
        "        \n",
        "    def save_params(self, file_name=\"params.pkl\"):\n",
        "        params = {}\n",
        "        for key, val in self.params.items():\n",
        "            params[key] = val\n",
        "        with open(file_name, 'wb') as f:\n",
        "            pickle.dump(params, f)\n",
        "\n",
        "    def load_params(self, file_name=\"params.pkl\"):\n",
        "        with open(file_name, 'rb') as f:\n",
        "            params = pickle.load(f)\n",
        "        for key, val in params.items():\n",
        "            self.params[key] = val\n",
        "\n",
        "        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n",
        "            self.layers[key].W = self.params['W' + str(i+1)]\n",
        "            self.layers[key].b = self.params['b' + str(i+1)]\n",
        "\n",
        "\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
        "\n",
        "\n",
        "#x_train, t_train = x_train[:5000], t_train[:5000]\n",
        "#x_test, t_test = x_test[:1000], t_test[:1000]\n",
        "\n",
        "max_epochs = 3   #20\n",
        "\n",
        "network = SimpleConvNet(input_dim=(1,28,28), \n",
        "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
        "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
        "                        \n",
        "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
        "                  epochs=max_epochs, mini_batch_size=100,\n",
        "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
        "                  evaluate_sample_num_per_epoch=1000)\n",
        "trainer.train()\n",
        "\n",
        "\n",
        "network.save_params(\"params.pkl\")\n",
        "print(\"Saved Network Parameters!\")\n",
        "\n",
        "\n",
        "markers = {'train': 'o', 'test': 's'}\n",
        "x = np.arange(max_epochs)\n",
        "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
        "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.ylim(0, 1.0)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading train-images-idx3-ubyte.gz ... \n",
            "Done\n",
            "Downloading train-labels-idx1-ubyte.gz ... \n",
            "Done\n",
            "Downloading t10k-images-idx3-ubyte.gz ... \n",
            "Done\n",
            "Downloading t10k-labels-idx1-ubyte.gz ... \n",
            "Done\n",
            "Converting train-images-idx3-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Converting train-labels-idx1-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Converting t10k-images-idx3-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Converting t10k-labels-idx1-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Creating pickle file ...\n",
            "Done!\n",
            "train loss:2.2990454404312652\n",
            "=== epoch:1, train acc:0.204, test acc:0.215 ===\n",
            "train loss:2.2958306091964182\n",
            "train loss:2.2911928434028037\n",
            "train loss:2.2796281674418997\n",
            "train loss:2.274052230327574\n",
            "train loss:2.2545091845877874\n",
            "train loss:2.2564803686216006\n",
            "train loss:2.2335874377172256\n",
            "train loss:2.193603037684353\n",
            "train loss:2.1537326843207247\n",
            "train loss:2.1124844012840707\n",
            "train loss:2.106646328092552\n",
            "train loss:2.0365081254786834\n",
            "train loss:2.0076006918095564\n",
            "train loss:1.9762636038900285\n",
            "train loss:1.9424328191491964\n",
            "train loss:1.7029482307021313\n",
            "train loss:1.684289702036786\n",
            "train loss:1.699340690801992\n",
            "train loss:1.5673416144367005\n",
            "train loss:1.5133046473696452\n",
            "train loss:1.385154429552326\n",
            "train loss:1.2769888112874375\n",
            "train loss:1.2760044022796735\n",
            "train loss:1.2096461934582698\n",
            "train loss:1.3900370024398214\n",
            "train loss:1.078145239618784\n",
            "train loss:1.0371230230386752\n",
            "train loss:1.0809024742002622\n",
            "train loss:0.984385710735776\n",
            "train loss:0.9207909239261568\n",
            "train loss:0.958618858746227\n",
            "train loss:0.8193731233864127\n",
            "train loss:0.831419731756283\n",
            "train loss:0.9187534568347409\n",
            "train loss:0.8538179052255045\n",
            "train loss:0.7693644332118645\n",
            "train loss:0.6871732863721904\n",
            "train loss:0.7280050157894286\n",
            "train loss:0.7428583413392412\n",
            "train loss:0.5810069965796623\n",
            "train loss:0.6128457228988156\n",
            "train loss:0.5403674998364457\n",
            "train loss:0.5862975885152115\n",
            "train loss:0.5508315178080982\n",
            "train loss:0.5470670097508507\n",
            "train loss:0.5303245818258367\n",
            "train loss:0.6665534045211068\n",
            "train loss:0.6802580194866268\n",
            "train loss:0.45091747802433974\n",
            "train loss:0.44467986195048986\n",
            "train loss:0.676746555410336\n",
            "train loss:0.6912494956920208\n",
            "train loss:0.46393089116861147\n",
            "train loss:0.6461950047598662\n",
            "train loss:0.536440369427631\n",
            "train loss:0.6297063025528138\n",
            "train loss:0.4608312686171206\n",
            "train loss:0.5495321349820244\n",
            "train loss:0.48316447707755145\n",
            "train loss:0.510603895166687\n",
            "train loss:0.3961099574682522\n",
            "train loss:0.5570991417682566\n",
            "train loss:0.45824298063810454\n",
            "train loss:0.48126017834206997\n",
            "train loss:0.47763802668404737\n",
            "train loss:0.500229807433899\n",
            "train loss:0.4196362385491875\n",
            "train loss:0.3690974904782079\n",
            "train loss:0.3944588998383732\n",
            "train loss:0.4922834407571265\n",
            "train loss:0.5304002479363042\n",
            "train loss:0.31963486494057974\n",
            "train loss:0.4243089887672726\n",
            "train loss:0.44914586921298416\n",
            "train loss:0.36832659792124356\n",
            "train loss:0.28828742366222\n",
            "train loss:0.4862243478449377\n",
            "train loss:0.2894957984981838\n",
            "train loss:0.5639416778970785\n",
            "train loss:0.27770403854494197\n",
            "train loss:0.40851493890554463\n",
            "train loss:0.25133895977650533\n",
            "train loss:0.4943031448109841\n",
            "train loss:0.5152722444060055\n",
            "train loss:0.6003460281866053\n",
            "train loss:0.5132540331087787\n",
            "train loss:0.4232789342975365\n",
            "train loss:0.43891388783523416\n",
            "train loss:0.2695484109646065\n",
            "train loss:0.33158963975944716\n",
            "train loss:0.25993830942989204\n",
            "train loss:0.605124217904199\n",
            "train loss:0.5475886616895427\n",
            "train loss:0.3100080847437351\n",
            "train loss:0.2808238233981058\n",
            "train loss:0.45053034387558377\n",
            "train loss:0.5759156562957172\n",
            "train loss:0.3669518766609113\n",
            "train loss:0.4417189948602229\n",
            "train loss:0.5041711798821455\n",
            "train loss:0.3549942477958964\n",
            "train loss:0.4553163594717832\n",
            "train loss:0.4269653846415704\n",
            "train loss:0.31070964487310154\n",
            "train loss:0.42940619415447295\n",
            "train loss:0.3782915652949738\n",
            "train loss:0.4433324536560241\n",
            "train loss:0.34421764448753117\n",
            "train loss:0.4596837828261686\n",
            "train loss:0.4976793887515108\n",
            "train loss:0.3382572325298338\n",
            "train loss:0.4312004609909385\n",
            "train loss:0.6122888696035526\n",
            "train loss:0.29059594344704487\n",
            "train loss:0.3775475346543616\n",
            "train loss:0.41276863944003084\n",
            "train loss:0.33002866721946966\n",
            "train loss:0.26673072783448754\n",
            "train loss:0.3421294732997406\n",
            "train loss:0.42299684046004077\n",
            "train loss:0.4510490211991411\n",
            "train loss:0.33026148772661945\n",
            "train loss:0.4204190474350945\n",
            "train loss:0.3762024326034508\n",
            "train loss:0.34047961978345087\n",
            "train loss:0.445670307695352\n",
            "train loss:0.27998322743657206\n",
            "train loss:0.49932479979711347\n",
            "train loss:0.3316661204689165\n",
            "train loss:0.3350327412002987\n",
            "train loss:0.43906076563997504\n",
            "train loss:0.287033154337731\n",
            "train loss:0.26928601811857217\n",
            "train loss:0.4227013113305576\n",
            "train loss:0.4114067080219004\n",
            "train loss:0.2920324939142955\n",
            "train loss:0.4270248504551335\n",
            "train loss:0.4665782002465411\n",
            "train loss:0.369313977765205\n",
            "train loss:0.306768730510815\n",
            "train loss:0.5446096271601344\n",
            "train loss:0.40161401724445545\n",
            "train loss:0.2668758405235374\n",
            "train loss:0.34927519334256096\n",
            "train loss:0.22320347471927338\n",
            "train loss:0.5002537639759828\n",
            "train loss:0.46575893206684954\n",
            "train loss:0.3870638056585487\n",
            "train loss:0.22832104828766792\n",
            "train loss:0.31679490617047273\n",
            "train loss:0.2836435089582021\n",
            "train loss:0.2625455052544934\n",
            "train loss:0.32482096214249295\n",
            "train loss:0.29663896026973036\n",
            "train loss:0.4264445961016361\n",
            "train loss:0.3465895901519038\n",
            "train loss:0.2648028817486243\n",
            "train loss:0.23739637256739424\n",
            "train loss:0.2634865904438618\n",
            "train loss:0.2255249411663558\n",
            "train loss:0.32437182603167414\n",
            "train loss:0.4511432643382743\n",
            "train loss:0.1994760183042118\n",
            "train loss:0.26000010953883007\n",
            "train loss:0.30318584154917794\n",
            "train loss:0.36510015022633246\n",
            "train loss:0.3113490992712459\n",
            "train loss:0.38454602811298727\n",
            "train loss:0.2523691652765105\n",
            "train loss:0.2718554854084504\n",
            "train loss:0.18847062777966683\n",
            "train loss:0.18144634656372255\n",
            "train loss:0.23322013283901896\n",
            "train loss:0.36812002669339594\n",
            "train loss:0.3152518892904903\n",
            "train loss:0.24132244586694893\n",
            "train loss:0.21496592969867973\n",
            "train loss:0.26377220579141447\n",
            "train loss:0.30077637603532764\n",
            "train loss:0.3700093707451096\n",
            "train loss:0.47403152258630726\n",
            "train loss:0.2705863431229024\n",
            "train loss:0.21232159572976617\n",
            "train loss:0.37514311264769273\n",
            "train loss:0.2989927199626789\n",
            "train loss:0.4556673208946651\n",
            "train loss:0.29022141590286504\n",
            "train loss:0.2250068687426746\n",
            "train loss:0.33462930311974115\n",
            "train loss:0.30630240670442593\n",
            "train loss:0.2112375366621977\n",
            "train loss:0.17612866623059897\n",
            "train loss:0.29955832120543613\n",
            "train loss:0.3566982642348456\n",
            "train loss:0.17740438902852693\n",
            "train loss:0.30311326349503565\n",
            "train loss:0.2441199592972191\n",
            "train loss:0.2576121777765852\n",
            "train loss:0.35314742969666585\n",
            "train loss:0.19747973419844558\n",
            "train loss:0.23371118345577024\n",
            "train loss:0.23971110684462082\n",
            "train loss:0.2910243124465133\n",
            "train loss:0.285419032424024\n",
            "train loss:0.3273730136319218\n",
            "train loss:0.1984387149303117\n",
            "train loss:0.19435207384394299\n",
            "train loss:0.2684126341344417\n",
            "train loss:0.30159821976310247\n",
            "train loss:0.23421292121937157\n",
            "train loss:0.33959630221448733\n",
            "train loss:0.2832954547470779\n",
            "train loss:0.15350733230377425\n",
            "train loss:0.3514790099110925\n",
            "train loss:0.25548956937344214\n",
            "train loss:0.2022652231013423\n",
            "train loss:0.24635435695251418\n",
            "train loss:0.3448627366279571\n",
            "train loss:0.361243732221568\n",
            "train loss:0.3523257429635599\n",
            "train loss:0.29224905489657244\n",
            "train loss:0.17233740375264575\n",
            "train loss:0.3363525593238681\n",
            "train loss:0.175929015567872\n",
            "train loss:0.2151639258226162\n",
            "train loss:0.24010523844879347\n",
            "train loss:0.28985648660581265\n",
            "train loss:0.42128112312013594\n",
            "train loss:0.20299639960861934\n",
            "train loss:0.10550225973445115\n",
            "train loss:0.14258557431606173\n",
            "train loss:0.2522705334131608\n",
            "train loss:0.2532230092062305\n",
            "train loss:0.26573298656518174\n",
            "train loss:0.2665574989684865\n",
            "train loss:0.3303313841261108\n",
            "train loss:0.2183445858764872\n",
            "train loss:0.20136252127123966\n",
            "train loss:0.3484247588049277\n",
            "train loss:0.29840557537993295\n",
            "train loss:0.20986239193832984\n",
            "train loss:0.20628809285539065\n",
            "train loss:0.2636784127752281\n",
            "train loss:0.3088975447561763\n",
            "train loss:0.36587648374143866\n",
            "train loss:0.24164043305420158\n",
            "train loss:0.1275197320148642\n",
            "train loss:0.25521527749733897\n",
            "train loss:0.35825063639590377\n",
            "train loss:0.263074199466135\n",
            "train loss:0.17727231767811047\n",
            "train loss:0.26439645413803553\n",
            "train loss:0.1655585072047677\n",
            "train loss:0.17392255857004202\n",
            "train loss:0.23282047003972559\n",
            "train loss:0.22676565007607968\n",
            "train loss:0.329668329403748\n",
            "train loss:0.30894327600815336\n",
            "train loss:0.2827497442452644\n",
            "train loss:0.24992335751120176\n",
            "train loss:0.19980340985951595\n",
            "train loss:0.21036832604238437\n",
            "train loss:0.16457208438415713\n",
            "train loss:0.5221989173289214\n",
            "train loss:0.44992189652731185\n",
            "train loss:0.22154260623081887\n",
            "train loss:0.2572545241705971\n",
            "train loss:0.19380271774770075\n",
            "train loss:0.28899082480932636\n",
            "train loss:0.3340786564663489\n",
            "train loss:0.2815760374843724\n",
            "train loss:0.16230536414996982\n",
            "train loss:0.2658274935565994\n",
            "train loss:0.24788823432421261\n",
            "train loss:0.22540678597327257\n",
            "train loss:0.2173751777058304\n",
            "train loss:0.21368784312208422\n",
            "train loss:0.22092584740503432\n",
            "train loss:0.19195169895877784\n",
            "train loss:0.16638760568014732\n",
            "train loss:0.1082408713071623\n",
            "train loss:0.2787497325127867\n",
            "train loss:0.25858532791752153\n",
            "train loss:0.14299455055614338\n",
            "train loss:0.1927568588889509\n",
            "train loss:0.14718577344278289\n",
            "train loss:0.17444114929873422\n",
            "train loss:0.29071770654477297\n",
            "train loss:0.22930416063983095\n",
            "train loss:0.16968572326535342\n",
            "train loss:0.2535603110492089\n",
            "train loss:0.1732453341380092\n",
            "train loss:0.24363168155289774\n",
            "train loss:0.1824425700683554\n",
            "train loss:0.2798506956168237\n",
            "train loss:0.3229543439563131\n",
            "train loss:0.2539944292066266\n",
            "train loss:0.1402558150081618\n",
            "train loss:0.12403272584355626\n",
            "train loss:0.18985515916740106\n",
            "train loss:0.323848583553597\n",
            "train loss:0.165019067031973\n",
            "train loss:0.2214700370735007\n",
            "train loss:0.16255146785527966\n",
            "train loss:0.1910966278684218\n",
            "train loss:0.4797260383985308\n",
            "train loss:0.17906072731229308\n",
            "train loss:0.20156381630606926\n",
            "train loss:0.1373938768649712\n",
            "train loss:0.3197445497685554\n",
            "train loss:0.24239648523449472\n",
            "train loss:0.2602544301964449\n",
            "train loss:0.28116317914366556\n",
            "train loss:0.2336764919805492\n",
            "train loss:0.09947605410433435\n",
            "train loss:0.2271513965946787\n",
            "train loss:0.24177944494539205\n",
            "train loss:0.1529592808129091\n",
            "train loss:0.18704178283127848\n",
            "train loss:0.15887285028034645\n",
            "train loss:0.23450039622759872\n",
            "train loss:0.1016460383701202\n",
            "train loss:0.280636991979639\n",
            "train loss:0.20607193509974775\n",
            "train loss:0.23001449738948668\n",
            "train loss:0.1716154171466033\n",
            "train loss:0.29529046396906206\n",
            "train loss:0.18104650125854435\n",
            "train loss:0.17831061766621248\n",
            "train loss:0.1402405554093174\n",
            "train loss:0.21082824644125023\n",
            "train loss:0.3187317590515894\n",
            "train loss:0.14646825084777895\n",
            "train loss:0.222643176800734\n",
            "train loss:0.1785398069506591\n",
            "train loss:0.34245996408179197\n",
            "train loss:0.2376473008176281\n",
            "train loss:0.16656666653519253\n",
            "train loss:0.18248931494710027\n",
            "train loss:0.31145963986012404\n",
            "train loss:0.1860472326940962\n",
            "train loss:0.21837644086605593\n",
            "train loss:0.15813133929973883\n",
            "train loss:0.07845182857250924\n",
            "train loss:0.16907281094159482\n",
            "train loss:0.3116739346956974\n",
            "train loss:0.17635381460913785\n",
            "train loss:0.09835531768218915\n",
            "train loss:0.20541360967375566\n",
            "train loss:0.20157681203989536\n",
            "train loss:0.13563600090087133\n",
            "train loss:0.3242479894180276\n",
            "train loss:0.10185296305084413\n",
            "train loss:0.2224198806819601\n",
            "train loss:0.22061946223919532\n",
            "train loss:0.28819486470032174\n",
            "train loss:0.1673484626296782\n",
            "train loss:0.2136758261057404\n",
            "train loss:0.12555138551505668\n",
            "train loss:0.1917943981603086\n",
            "train loss:0.1984505208777211\n",
            "train loss:0.1933504558602244\n",
            "train loss:0.09007286270512743\n",
            "train loss:0.14211711776829467\n",
            "train loss:0.25968971041364036\n",
            "train loss:0.2192405662539081\n",
            "train loss:0.17938819091239036\n",
            "train loss:0.14033153092164916\n",
            "train loss:0.26321171067747556\n",
            "train loss:0.23978821615325663\n",
            "train loss:0.08285844317706008\n",
            "train loss:0.2659244781365791\n",
            "train loss:0.14121703001294006\n",
            "train loss:0.26404727590299926\n",
            "train loss:0.05253090710060263\n",
            "train loss:0.16389586187962066\n",
            "train loss:0.3062685201771995\n",
            "train loss:0.1928580561317566\n",
            "train loss:0.31028072987067395\n",
            "train loss:0.13719505983490834\n",
            "train loss:0.29124603634161816\n",
            "train loss:0.253115887674673\n",
            "train loss:0.0981138673614592\n",
            "train loss:0.124785223016673\n",
            "train loss:0.12947085957454393\n",
            "train loss:0.07956863050690735\n",
            "train loss:0.17113664641305615\n",
            "train loss:0.1684424468589171\n",
            "train loss:0.25241490003934874\n",
            "train loss:0.14158959528173803\n",
            "train loss:0.13722382935891841\n",
            "train loss:0.09707925234453153\n",
            "train loss:0.11337070056367765\n",
            "train loss:0.09672695798784656\n",
            "train loss:0.08600043524401897\n",
            "train loss:0.24223015011035187\n",
            "train loss:0.1743670754871431\n",
            "train loss:0.13167310379206612\n",
            "train loss:0.24028229738911341\n",
            "train loss:0.20673912461609162\n",
            "train loss:0.29934113811034296\n",
            "train loss:0.10243502216466799\n",
            "train loss:0.23530004755138179\n",
            "train loss:0.10590750207810591\n",
            "train loss:0.21207303004239886\n",
            "train loss:0.2026358628031252\n",
            "train loss:0.10410723799621621\n",
            "train loss:0.16531358683813152\n",
            "train loss:0.24826000006628782\n",
            "train loss:0.11385160757388971\n",
            "train loss:0.16739764716999453\n",
            "train loss:0.17913231761505893\n",
            "train loss:0.2655468004072336\n",
            "train loss:0.1246877752516218\n",
            "train loss:0.30087114384356306\n",
            "train loss:0.1827610215211156\n",
            "train loss:0.1282476036247302\n",
            "train loss:0.2769669525874646\n",
            "train loss:0.22011015019015745\n",
            "train loss:0.08894531465768962\n",
            "train loss:0.263736480461012\n",
            "train loss:0.0965153903109631\n",
            "train loss:0.1649538064937596\n",
            "train loss:0.1187821669215646\n",
            "train loss:0.19676605217533577\n",
            "train loss:0.1329480437538611\n",
            "train loss:0.14655865472278334\n",
            "train loss:0.1612473403726991\n",
            "train loss:0.19882441176456733\n",
            "train loss:0.12143648132759365\n",
            "train loss:0.14385647844020505\n",
            "train loss:0.17321559896214125\n",
            "train loss:0.17291977060210015\n",
            "train loss:0.19739844108240895\n",
            "train loss:0.1787193453447606\n",
            "train loss:0.36491596997599557\n",
            "train loss:0.08734841366157435\n",
            "train loss:0.16210300013486661\n",
            "train loss:0.10450218999514214\n",
            "train loss:0.1572048839553718\n",
            "train loss:0.1562783736019393\n",
            "train loss:0.24446466245830106\n",
            "train loss:0.11071054486827245\n",
            "train loss:0.1552418665784961\n",
            "train loss:0.0977186885925441\n",
            "train loss:0.21345210778491439\n",
            "train loss:0.20158002532794175\n",
            "train loss:0.10163561305919369\n",
            "train loss:0.11571748029380363\n",
            "train loss:0.23314644182548488\n",
            "train loss:0.14298175765689672\n",
            "train loss:0.30011797929858197\n",
            "train loss:0.09883249994035412\n",
            "train loss:0.11399439532123852\n",
            "train loss:0.15645107043206902\n",
            "train loss:0.19093912402046395\n",
            "train loss:0.11711621172989614\n",
            "train loss:0.13383534557941615\n",
            "train loss:0.07351434572862854\n",
            "train loss:0.09784185668463356\n",
            "train loss:0.10284149278571643\n",
            "train loss:0.053069183304011405\n",
            "train loss:0.20171738143801948\n",
            "train loss:0.18607192013703344\n",
            "train loss:0.12314033559600786\n",
            "train loss:0.10583321601862135\n",
            "train loss:0.14549768275980543\n",
            "train loss:0.12991867220019976\n",
            "train loss:0.11627149272312354\n",
            "train loss:0.1114545175485267\n",
            "train loss:0.12048366796856183\n",
            "train loss:0.1607810915009562\n",
            "train loss:0.09560000503204394\n",
            "train loss:0.1285249207649212\n",
            "train loss:0.22798905913847245\n",
            "train loss:0.20718038266797606\n",
            "train loss:0.18968910654206358\n",
            "train loss:0.09680907227384637\n",
            "train loss:0.1770144960688403\n",
            "train loss:0.2114660341250809\n",
            "train loss:0.15910628947705377\n",
            "train loss:0.08570622230902229\n",
            "train loss:0.24619552822006743\n",
            "train loss:0.060618057327971535\n",
            "train loss:0.169901333411815\n",
            "train loss:0.15718080564243544\n",
            "train loss:0.09848508454837092\n",
            "train loss:0.15029680588118505\n",
            "train loss:0.20509740659131526\n",
            "train loss:0.14336320696148439\n",
            "train loss:0.1907829845024023\n",
            "train loss:0.16091503050221045\n",
            "train loss:0.12946876704247068\n",
            "train loss:0.0625341195428551\n",
            "train loss:0.11771304592113449\n",
            "train loss:0.21626973820758633\n",
            "train loss:0.10517832386034916\n",
            "train loss:0.1679316526103101\n",
            "train loss:0.0724111322601651\n",
            "train loss:0.06323310465452417\n",
            "train loss:0.12798871087681685\n",
            "train loss:0.22152992375002092\n",
            "train loss:0.25943296713994235\n",
            "train loss:0.0886402421471255\n",
            "train loss:0.09408222890944481\n",
            "train loss:0.07555390761518592\n",
            "train loss:0.09939947634245291\n",
            "train loss:0.1580100962742196\n",
            "train loss:0.16958550732131653\n",
            "train loss:0.1367782691169491\n",
            "train loss:0.11918770557949694\n",
            "train loss:0.09765124502447611\n",
            "train loss:0.20034786947037392\n",
            "train loss:0.23563768808240734\n",
            "train loss:0.15824915126328729\n",
            "train loss:0.1338349361829904\n",
            "train loss:0.08524730095195658\n",
            "train loss:0.1376054263792925\n",
            "train loss:0.1930752274062672\n",
            "train loss:0.18210656188270413\n",
            "train loss:0.18451877595206473\n",
            "train loss:0.14787180714677228\n",
            "train loss:0.11820966989240209\n",
            "train loss:0.07521985623187412\n",
            "train loss:0.18141291558142988\n",
            "train loss:0.16737415021515312\n",
            "train loss:0.17006959207177297\n",
            "train loss:0.07472724974647221\n",
            "train loss:0.11498723798666008\n",
            "train loss:0.10452541095745504\n",
            "train loss:0.19313146715382334\n",
            "train loss:0.13014740533853733\n",
            "train loss:0.056412271973879824\n",
            "train loss:0.2692667519140343\n",
            "train loss:0.15619464990823118\n",
            "train loss:0.1266134206482128\n",
            "train loss:0.25110549077472405\n",
            "train loss:0.0852947750274839\n",
            "train loss:0.1341807939901384\n",
            "train loss:0.10106857234433958\n",
            "train loss:0.2135176276087959\n",
            "train loss:0.11215788001509773\n",
            "train loss:0.11269647605368444\n",
            "train loss:0.1660104958133568\n",
            "train loss:0.10625712097401564\n",
            "train loss:0.10703606734379957\n",
            "train loss:0.13876522069088257\n",
            "train loss:0.12976682054421695\n",
            "train loss:0.07232007805990431\n",
            "train loss:0.09374635609092762\n",
            "train loss:0.17864354911730332\n",
            "train loss:0.09234229745003841\n",
            "train loss:0.1322673321540386\n",
            "train loss:0.11313913042571729\n",
            "train loss:0.16627769728723663\n",
            "train loss:0.1356048173000298\n",
            "train loss:0.02777155778668432\n",
            "train loss:0.14998544866626307\n",
            "train loss:0.08523161351474451\n",
            "train loss:0.17108307157384814\n",
            "train loss:0.16498450224422986\n",
            "train loss:0.1077564628772661\n",
            "train loss:0.10513269738724236\n",
            "train loss:0.1443078650799399\n",
            "train loss:0.12532332655491277\n",
            "train loss:0.07941920062464898\n",
            "train loss:0.03790262078723536\n",
            "train loss:0.24474574740716548\n",
            "train loss:0.25508589829162903\n",
            "train loss:0.05836907699895118\n",
            "train loss:0.08661529952519116\n",
            "train loss:0.13274038408246458\n",
            "train loss:0.06609652625647339\n",
            "train loss:0.18502622890370776\n",
            "train loss:0.06675556434683111\n",
            "train loss:0.07062349240513817\n",
            "train loss:0.08850841688436083\n",
            "train loss:0.07231545126587416\n",
            "train loss:0.0596596789569436\n",
            "train loss:0.10332143337382832\n",
            "train loss:0.0533890589225327\n",
            "train loss:0.1444520214327785\n",
            "train loss:0.1878955905917746\n",
            "train loss:0.08961498916240312\n",
            "train loss:0.07572468048355206\n",
            "train loss:0.07778397043193838\n",
            "train loss:0.09707557298131246\n",
            "train loss:0.06520369849459183\n",
            "train loss:0.10978341019759225\n",
            "train loss:0.06882171422463398\n",
            "train loss:0.1309090244098541\n",
            "train loss:0.07078197128874036\n",
            "train loss:0.1469594529126897\n",
            "train loss:0.13137609005362352\n",
            "train loss:0.14749602925655397\n",
            "train loss:0.19106089665848977\n",
            "train loss:0.04910661478804105\n",
            "train loss:0.2616534443959327\n",
            "train loss:0.07487807306688499\n",
            "train loss:0.10582974050241324\n",
            "=== epoch:2, train acc:0.966, test acc:0.956 ===\n",
            "train loss:0.053935995087454745\n",
            "train loss:0.10975919436387713\n",
            "train loss:0.1089698429737252\n",
            "train loss:0.08540099352277546\n",
            "train loss:0.10507132107238153\n",
            "train loss:0.17982031486949665\n",
            "train loss:0.09119264428434061\n",
            "train loss:0.035386115844626934\n",
            "train loss:0.20306637083254425\n",
            "train loss:0.08158951697740253\n",
            "train loss:0.19530543424244373\n",
            "train loss:0.054369236240534004\n",
            "train loss:0.08400113044058971\n",
            "train loss:0.03712804391406508\n",
            "train loss:0.12238504502407839\n",
            "train loss:0.05536301276339102\n",
            "train loss:0.10403741127607175\n",
            "train loss:0.21751560111093013\n",
            "train loss:0.03795504204197904\n",
            "train loss:0.1839582877775002\n",
            "train loss:0.2937206154231091\n",
            "train loss:0.1332135822465385\n",
            "train loss:0.10472342464408495\n",
            "train loss:0.18849287832719008\n",
            "train loss:0.169545257149464\n",
            "train loss:0.1662469901427377\n",
            "train loss:0.14420014405969883\n",
            "train loss:0.08070735026487862\n",
            "train loss:0.1309214089880621\n",
            "train loss:0.1703584538243918\n",
            "train loss:0.0982894480371994\n",
            "train loss:0.07377618056999276\n",
            "train loss:0.139837581621179\n",
            "train loss:0.0726277277789915\n",
            "train loss:0.06516959501931796\n",
            "train loss:0.181819750272182\n",
            "train loss:0.06807560331660702\n",
            "train loss:0.3020538856156821\n",
            "train loss:0.13413124922624461\n",
            "train loss:0.17020991440241626\n",
            "train loss:0.16757759844735926\n",
            "train loss:0.14526157647896243\n",
            "train loss:0.10470149701246127\n",
            "train loss:0.0746152468394031\n",
            "train loss:0.1194230969475508\n",
            "train loss:0.22688173149305021\n",
            "train loss:0.0870563596715147\n",
            "train loss:0.07922949033689504\n",
            "train loss:0.14439115332488894\n",
            "train loss:0.06488437540439923\n",
            "train loss:0.09172813102474892\n",
            "train loss:0.19202510294071862\n",
            "train loss:0.08266327777788461\n",
            "train loss:0.07847392673686167\n",
            "train loss:0.09953721278040455\n",
            "train loss:0.06707332604869883\n",
            "train loss:0.12658936302611257\n",
            "train loss:0.2035051576332539\n",
            "train loss:0.11888946897204362\n",
            "train loss:0.1278705968063626\n",
            "train loss:0.11868052068594376\n",
            "train loss:0.2854049020532189\n",
            "train loss:0.0540392652324105\n",
            "train loss:0.10970057450290038\n",
            "train loss:0.14304577170395508\n",
            "train loss:0.07885828713204507\n",
            "train loss:0.050836714116938325\n",
            "train loss:0.11053337305504835\n",
            "train loss:0.08537078442840774\n",
            "train loss:0.033512543489991005\n",
            "train loss:0.11019577996690684\n",
            "train loss:0.13309542453401838\n",
            "train loss:0.06821084716393754\n",
            "train loss:0.13722540643867714\n",
            "train loss:0.07476169229643012\n",
            "train loss:0.03433190607036431\n",
            "train loss:0.12024728163728125\n",
            "train loss:0.12432461386526017\n",
            "train loss:0.126597101397449\n",
            "train loss:0.06209316257180022\n",
            "train loss:0.08057321601894142\n",
            "train loss:0.09798227890412176\n",
            "train loss:0.12709155160369517\n",
            "train loss:0.06482422978932836\n",
            "train loss:0.11131612967198948\n",
            "train loss:0.1392655285113395\n",
            "train loss:0.14828000366220892\n",
            "train loss:0.07354192512529074\n",
            "train loss:0.22433843761312147\n",
            "train loss:0.10593819198349547\n",
            "train loss:0.17629937076925678\n",
            "train loss:0.08015149528834443\n",
            "train loss:0.05498913909269845\n",
            "train loss:0.05294549141509987\n",
            "train loss:0.14625736835441588\n",
            "train loss:0.07141250323982161\n",
            "train loss:0.09382091498063888\n",
            "train loss:0.07907404393847606\n",
            "train loss:0.087294486353771\n",
            "train loss:0.09041481498973646\n",
            "train loss:0.11179184339659427\n",
            "train loss:0.10368285969547926\n",
            "train loss:0.0601103939392965\n",
            "train loss:0.15155640023555883\n",
            "train loss:0.1335024903137821\n",
            "train loss:0.05432393233910475\n",
            "train loss:0.036120902573899875\n",
            "train loss:0.06326628104281547\n",
            "train loss:0.08205249690192\n",
            "train loss:0.1452023909235679\n",
            "train loss:0.0716714331556874\n",
            "train loss:0.10851044865243178\n",
            "train loss:0.13165797428544002\n",
            "train loss:0.102474569732321\n",
            "train loss:0.10961457649617877\n",
            "train loss:0.13902969953599503\n",
            "train loss:0.2164773095415039\n",
            "train loss:0.08312436948536718\n",
            "train loss:0.1312204345578347\n",
            "train loss:0.05390801865575127\n",
            "train loss:0.15858355471300828\n",
            "train loss:0.10969863885691962\n",
            "train loss:0.1153816531706613\n",
            "train loss:0.07258458478459662\n",
            "train loss:0.07469304104488163\n",
            "train loss:0.08280765303206516\n",
            "train loss:0.18287033795129073\n",
            "train loss:0.12867141455147457\n",
            "train loss:0.10857078640234978\n",
            "train loss:0.07162474266126068\n",
            "train loss:0.10217692954438673\n",
            "train loss:0.22196046811044312\n",
            "train loss:0.09420378331062546\n",
            "train loss:0.03531114498438563\n",
            "train loss:0.11363175983290386\n",
            "train loss:0.10563912424159766\n",
            "train loss:0.11858265295031657\n",
            "train loss:0.21447176059214498\n",
            "train loss:0.08691744923319852\n",
            "train loss:0.13464457908339653\n",
            "train loss:0.07616107311173455\n",
            "train loss:0.13937984436444237\n",
            "train loss:0.11950041219759314\n",
            "train loss:0.0880455881624577\n",
            "train loss:0.1812208306581189\n",
            "train loss:0.1340006878603708\n",
            "train loss:0.07609403829115646\n",
            "train loss:0.0607402946529478\n",
            "train loss:0.10156115668115032\n",
            "train loss:0.1572257159139819\n",
            "train loss:0.11699330183777276\n",
            "train loss:0.12872761876267597\n",
            "train loss:0.10436539748591973\n",
            "train loss:0.07459311791895974\n",
            "train loss:0.1324253870964063\n",
            "train loss:0.07860696591509492\n",
            "train loss:0.05966368225927122\n",
            "train loss:0.05174650488787374\n",
            "train loss:0.13400868892235057\n",
            "train loss:0.1467071118468847\n",
            "train loss:0.07713759915827242\n",
            "train loss:0.08858724109332869\n",
            "train loss:0.08066061588272998\n",
            "train loss:0.08318755247897325\n",
            "train loss:0.08210026552046917\n",
            "train loss:0.06921977737676646\n",
            "train loss:0.11418310252290252\n",
            "train loss:0.07100730422033232\n",
            "train loss:0.10175522813112739\n",
            "train loss:0.08985975360370514\n",
            "train loss:0.0979790974519692\n",
            "train loss:0.16441732106298118\n",
            "train loss:0.046894862915859425\n",
            "train loss:0.06453677427409575\n",
            "train loss:0.16057764659149754\n",
            "train loss:0.11868941889984377\n",
            "train loss:0.12152527196563892\n",
            "train loss:0.06825237214014752\n",
            "train loss:0.04726260111013939\n",
            "train loss:0.2091895622264949\n",
            "train loss:0.06008810622497227\n",
            "train loss:0.07641081791769802\n",
            "train loss:0.07415306620344901\n",
            "train loss:0.021571734673419066\n",
            "train loss:0.1323618909889977\n",
            "train loss:0.14760740137833792\n",
            "train loss:0.06570678125844352\n",
            "train loss:0.08240357788441643\n",
            "train loss:0.057068669737679406\n",
            "train loss:0.12342568633396554\n",
            "train loss:0.03731806622658168\n",
            "train loss:0.10730416478531046\n",
            "train loss:0.0606280968152333\n",
            "train loss:0.19960930552525064\n",
            "train loss:0.10349223398316904\n",
            "train loss:0.08801975280632492\n",
            "train loss:0.14720938825721863\n",
            "train loss:0.08737323529494267\n",
            "train loss:0.07205898652843222\n",
            "train loss:0.03464144693098325\n",
            "train loss:0.09873578683054898\n",
            "train loss:0.08993369871446659\n",
            "train loss:0.09106204469713519\n",
            "train loss:0.10378504701037564\n",
            "train loss:0.16436455804170177\n",
            "train loss:0.10049580402803697\n",
            "train loss:0.06967315023131779\n",
            "train loss:0.0695541938121302\n",
            "train loss:0.11518179686932842\n",
            "train loss:0.1646530798914291\n",
            "train loss:0.11556830713183322\n",
            "train loss:0.06935271680589179\n",
            "train loss:0.12415050467708122\n",
            "train loss:0.07572529326763221\n",
            "train loss:0.06785474520149452\n",
            "train loss:0.06289067065458061\n",
            "train loss:0.0875594425263207\n",
            "train loss:0.10823254865773191\n",
            "train loss:0.08244451341595736\n",
            "train loss:0.058006011915024386\n",
            "train loss:0.128162082155678\n",
            "train loss:0.18151694313181588\n",
            "train loss:0.15728889602144927\n",
            "train loss:0.09254991632373143\n",
            "train loss:0.13970604197716777\n",
            "train loss:0.12040283802135518\n",
            "train loss:0.12990428079748517\n",
            "train loss:0.04105268932871356\n",
            "train loss:0.15754799336533779\n",
            "train loss:0.1102761950184812\n",
            "train loss:0.1230689352860939\n",
            "train loss:0.09893616785912858\n",
            "train loss:0.08726461249407126\n",
            "train loss:0.07313647318547291\n",
            "train loss:0.1068892283863958\n",
            "train loss:0.06247646057554153\n",
            "train loss:0.08883338909112391\n",
            "train loss:0.09167007162356253\n",
            "train loss:0.0362875849689465\n",
            "train loss:0.06462952718426292\n",
            "train loss:0.05881081164620116\n",
            "train loss:0.07020608584844623\n",
            "train loss:0.048820698272023816\n",
            "train loss:0.08302189346223438\n",
            "train loss:0.1652537572803826\n",
            "train loss:0.021987772580905677\n",
            "train loss:0.05214786411176822\n",
            "train loss:0.16812923557234483\n",
            "train loss:0.1845314502057073\n",
            "train loss:0.08331419250727683\n",
            "train loss:0.18701338305446086\n",
            "train loss:0.06930730558723094\n",
            "train loss:0.11497257782763735\n",
            "train loss:0.021687947854760695\n",
            "train loss:0.11486497307793096\n",
            "train loss:0.021901292877489865\n",
            "train loss:0.052972103154945833\n",
            "train loss:0.0703595823920469\n",
            "train loss:0.14871999180049392\n",
            "train loss:0.058702322117411755\n",
            "train loss:0.08046125947505446\n",
            "train loss:0.05597575344018886\n",
            "train loss:0.12446380466041587\n",
            "train loss:0.0780599774707528\n",
            "train loss:0.05393671444946312\n",
            "train loss:0.08102646665676756\n",
            "train loss:0.04292162586480416\n",
            "train loss:0.08223107687602015\n",
            "train loss:0.06462112811891764\n",
            "train loss:0.08691490127201043\n",
            "train loss:0.07998510892782043\n",
            "train loss:0.13229441720796492\n",
            "train loss:0.10010660419564314\n",
            "train loss:0.05467973823896717\n",
            "train loss:0.07517119363800091\n",
            "train loss:0.10971075350585405\n",
            "train loss:0.07260001946657271\n",
            "train loss:0.08562183729633677\n",
            "train loss:0.1429868241573548\n",
            "train loss:0.15010564232818932\n",
            "train loss:0.09276324761302264\n",
            "train loss:0.05993953133796838\n",
            "train loss:0.04038371127536895\n",
            "train loss:0.1434245076155089\n",
            "train loss:0.1731823288464563\n",
            "train loss:0.06831476810860267\n",
            "train loss:0.12606412312253076\n",
            "train loss:0.11297946576146264\n",
            "train loss:0.06421101490153029\n",
            "train loss:0.10488485990110115\n",
            "train loss:0.13979549376686864\n",
            "train loss:0.050808210924220054\n",
            "train loss:0.04592889112345155\n",
            "train loss:0.05223149865719729\n",
            "train loss:0.03261416533763819\n",
            "train loss:0.07979633866623588\n",
            "train loss:0.06400259836846743\n",
            "train loss:0.04348369628525368\n",
            "train loss:0.08400155361057436\n",
            "train loss:0.05786377175642152\n",
            "train loss:0.10677894567553152\n",
            "train loss:0.017831648946139873\n",
            "train loss:0.04054635683246454\n",
            "train loss:0.06821272272559482\n",
            "train loss:0.03743739560134725\n",
            "train loss:0.1828102541216129\n",
            "train loss:0.07540561405478964\n",
            "train loss:0.05562664719556389\n",
            "train loss:0.12608828596209626\n",
            "train loss:0.047950220587493896\n",
            "train loss:0.10348030955784746\n",
            "train loss:0.05068879269463099\n",
            "train loss:0.06801359569575662\n",
            "train loss:0.19160947807430445\n",
            "train loss:0.09470813028621512\n",
            "train loss:0.025989515431145374\n",
            "train loss:0.05120054049928774\n",
            "train loss:0.09113367826982247\n",
            "train loss:0.09696015042083857\n",
            "train loss:0.07295637277843921\n",
            "train loss:0.07902330338116731\n",
            "train loss:0.07483110065025525\n",
            "train loss:0.06987792799997465\n",
            "train loss:0.04783986040333206\n",
            "train loss:0.13635685065629438\n",
            "train loss:0.05682702128069375\n",
            "train loss:0.06353414748951482\n",
            "train loss:0.17246194563904754\n",
            "train loss:0.09015939673316165\n",
            "train loss:0.11501661419972782\n",
            "train loss:0.07498397654865191\n",
            "train loss:0.036759094292285674\n",
            "train loss:0.056256115415856914\n",
            "train loss:0.03659550773532993\n",
            "train loss:0.049263081786811054\n",
            "train loss:0.0698784164786023\n",
            "train loss:0.06758471284526574\n",
            "train loss:0.25157001749100993\n",
            "train loss:0.06336642418806188\n",
            "train loss:0.08421557476872868\n",
            "train loss:0.036503444918243824\n",
            "train loss:0.0845135381420566\n",
            "train loss:0.037104921344571246\n",
            "train loss:0.07081846288461247\n",
            "train loss:0.10731102573818106\n",
            "train loss:0.11695241441885995\n",
            "train loss:0.09860322076876805\n",
            "train loss:0.06218450775967164\n",
            "train loss:0.07818646848750378\n",
            "train loss:0.061031438021450224\n",
            "train loss:0.018532193217511508\n",
            "train loss:0.08254530577295945\n",
            "train loss:0.04234495566723366\n",
            "train loss:0.09534325389357853\n",
            "train loss:0.05945979119678371\n",
            "train loss:0.04126122217133086\n",
            "train loss:0.04649461919594667\n",
            "train loss:0.2210491830637647\n",
            "train loss:0.17205048714756646\n",
            "train loss:0.04817120851493251\n",
            "train loss:0.14141915208448363\n",
            "train loss:0.064081421840529\n",
            "train loss:0.04816185954815178\n",
            "train loss:0.02556369730748269\n",
            "train loss:0.030069516773190884\n",
            "train loss:0.1463929076004578\n",
            "train loss:0.05122328262273963\n",
            "train loss:0.05744164756382016\n",
            "train loss:0.0879471034360666\n",
            "train loss:0.04522230112371346\n",
            "train loss:0.06049939011384208\n",
            "train loss:0.04806702871640283\n",
            "train loss:0.09861521918142412\n",
            "train loss:0.04099276809421246\n",
            "train loss:0.1431349713778895\n",
            "train loss:0.18767840919945708\n",
            "train loss:0.08312478470791922\n",
            "train loss:0.027509238758827506\n",
            "train loss:0.14503901745903433\n",
            "train loss:0.1361210430070934\n",
            "train loss:0.08449614923563194\n",
            "train loss:0.07737460335856708\n",
            "train loss:0.06617745657172004\n",
            "train loss:0.07330233113826658\n",
            "train loss:0.0885543954685585\n",
            "train loss:0.15438122572201168\n",
            "train loss:0.037929198003262395\n",
            "train loss:0.023672353554053692\n",
            "train loss:0.060285082458240574\n",
            "train loss:0.10382173914946416\n",
            "train loss:0.014854605639891681\n",
            "train loss:0.06583379678117726\n",
            "train loss:0.0569847321634071\n",
            "train loss:0.08460497118388602\n",
            "train loss:0.12734316627338368\n",
            "train loss:0.038569997750337495\n",
            "train loss:0.10807872899269406\n",
            "train loss:0.08134217755445658\n",
            "train loss:0.056431119560386954\n",
            "train loss:0.056969890973459086\n",
            "train loss:0.06574114871162898\n",
            "train loss:0.05961622514583806\n",
            "train loss:0.04671984957508776\n",
            "train loss:0.13887225803080094\n",
            "train loss:0.08099372778226907\n",
            "train loss:0.13771349808675729\n",
            "train loss:0.24794235950681226\n",
            "train loss:0.038298598009425366\n",
            "train loss:0.026556576179198984\n",
            "train loss:0.07084606084158301\n",
            "train loss:0.031494205779680075\n",
            "train loss:0.040576747341307416\n",
            "train loss:0.09029873981993002\n",
            "train loss:0.18689760294365743\n",
            "train loss:0.026936634929286777\n",
            "train loss:0.03658626973409072\n",
            "train loss:0.057565374904851\n",
            "train loss:0.1096974705036004\n",
            "train loss:0.05775504042661356\n",
            "train loss:0.06469630746789362\n",
            "train loss:0.13666107382121653\n",
            "train loss:0.015069357624778078\n",
            "train loss:0.04000521458725371\n",
            "train loss:0.08288100201590075\n",
            "train loss:0.10955587150504476\n",
            "train loss:0.04405042177946032\n",
            "train loss:0.07739413788768941\n",
            "train loss:0.04622835001952749\n",
            "train loss:0.07194566712413265\n",
            "train loss:0.09096142109925476\n",
            "train loss:0.055577352455927674\n",
            "train loss:0.04544779482892909\n",
            "train loss:0.027213427160373435\n",
            "train loss:0.08017144184409869\n",
            "train loss:0.022644303105939317\n",
            "train loss:0.031048065155992025\n",
            "train loss:0.03982786368302713\n",
            "train loss:0.032349399683981206\n",
            "train loss:0.10332000852015383\n",
            "train loss:0.029468757140881104\n",
            "train loss:0.14656560458197046\n",
            "train loss:0.061656635193859825\n",
            "train loss:0.048567808715348305\n",
            "train loss:0.04458177754069641\n",
            "train loss:0.03086659754177826\n",
            "train loss:0.05086228485676008\n",
            "train loss:0.027948506732071342\n",
            "train loss:0.14930511075678307\n",
            "train loss:0.09352903544637613\n",
            "train loss:0.04967359806924893\n",
            "train loss:0.09596849056196222\n",
            "train loss:0.03852848636534463\n",
            "train loss:0.07228064962481032\n",
            "train loss:0.0665723109149069\n",
            "train loss:0.013362757476549171\n",
            "train loss:0.056225620618998884\n",
            "train loss:0.06751799216667342\n",
            "train loss:0.06884890679399876\n",
            "train loss:0.06241259103137784\n",
            "train loss:0.0632325923994333\n",
            "train loss:0.09933828988643817\n",
            "train loss:0.13525243580161467\n",
            "train loss:0.08289193699027457\n",
            "train loss:0.031013583921443332\n",
            "train loss:0.16087656921835145\n",
            "train loss:0.027940029995437276\n",
            "train loss:0.05123985006412364\n",
            "train loss:0.04043188999172047\n",
            "train loss:0.08942658274282625\n",
            "train loss:0.041407639996256206\n",
            "train loss:0.051673707110257594\n",
            "train loss:0.03752653987601441\n",
            "train loss:0.04405880559300642\n",
            "train loss:0.17066861624468263\n",
            "train loss:0.0442020524951531\n",
            "train loss:0.07399987279351622\n",
            "train loss:0.047169270521898274\n",
            "train loss:0.03167075119624133\n",
            "train loss:0.15965627276009522\n",
            "train loss:0.07030773657688635\n",
            "train loss:0.1606721429844506\n",
            "train loss:0.07542038778818509\n",
            "train loss:0.08084763215541406\n",
            "train loss:0.04550395534705949\n",
            "train loss:0.03286687349077948\n",
            "train loss:0.051919864093577975\n",
            "train loss:0.07138103914657949\n",
            "train loss:0.07828794883127227\n",
            "train loss:0.04269138220474782\n",
            "train loss:0.049544675881993336\n",
            "train loss:0.07355082938315567\n",
            "train loss:0.025078379865167463\n",
            "train loss:0.07364002729426447\n",
            "train loss:0.02317253310224381\n",
            "train loss:0.07380974480713397\n",
            "train loss:0.06589574672103528\n",
            "train loss:0.04916889525974562\n",
            "train loss:0.1273238230907735\n",
            "train loss:0.05099743673881564\n",
            "train loss:0.04789872860986748\n",
            "train loss:0.033931107837796216\n",
            "train loss:0.0281311551875891\n",
            "train loss:0.08361475224600529\n",
            "train loss:0.08633829074434121\n",
            "train loss:0.018814570020657406\n",
            "train loss:0.09752777257823304\n",
            "train loss:0.10183274663972053\n",
            "train loss:0.03948017685116729\n",
            "train loss:0.036735244712770496\n",
            "train loss:0.05480479201882856\n",
            "train loss:0.03268905263250601\n",
            "train loss:0.09654886169155015\n",
            "train loss:0.08802446180488839\n",
            "train loss:0.14346870482624693\n",
            "train loss:0.12906403850946835\n",
            "train loss:0.10165563930977015\n",
            "train loss:0.02168095704696285\n",
            "train loss:0.07478704956652138\n",
            "train loss:0.12176707440241363\n",
            "train loss:0.08809195966404548\n",
            "train loss:0.05335816718040955\n",
            "train loss:0.07535664802312511\n",
            "train loss:0.04671154463391712\n",
            "train loss:0.11920960789743655\n",
            "train loss:0.05635549305232487\n",
            "train loss:0.050267148496779955\n",
            "train loss:0.037531854170297765\n",
            "train loss:0.04064345681903471\n",
            "train loss:0.06551306074012926\n",
            "train loss:0.09299328824290588\n",
            "train loss:0.028398784929078012\n",
            "train loss:0.12448329293661793\n",
            "train loss:0.07901830643209656\n",
            "train loss:0.047085904100874786\n",
            "train loss:0.09885153192427369\n",
            "train loss:0.039468372602945084\n",
            "train loss:0.13677469985252555\n",
            "train loss:0.09193385106007822\n",
            "train loss:0.055058221964864075\n",
            "train loss:0.07687636104384658\n",
            "train loss:0.052289523969519636\n",
            "train loss:0.0652562744275492\n",
            "train loss:0.02662882636519275\n",
            "train loss:0.038170416610803136\n",
            "train loss:0.04130616162484462\n",
            "train loss:0.08142217669146606\n",
            "train loss:0.0778728869031845\n",
            "train loss:0.025894379574122568\n",
            "train loss:0.140113197458584\n",
            "train loss:0.09290459542778719\n",
            "train loss:0.020121697178399817\n",
            "train loss:0.04568192506589815\n",
            "train loss:0.04863003582408718\n",
            "train loss:0.1972400620854182\n",
            "train loss:0.08661259173576258\n",
            "train loss:0.057236510228888066\n",
            "train loss:0.08538148877259612\n",
            "train loss:0.06291519055174524\n",
            "train loss:0.0243177218353296\n",
            "train loss:0.09137374666916306\n",
            "train loss:0.07218953689048523\n",
            "train loss:0.02260874219431524\n",
            "train loss:0.06551865672282423\n",
            "train loss:0.06366795129070325\n",
            "train loss:0.04009458687627945\n",
            "train loss:0.032702442323558074\n",
            "train loss:0.13637531708999956\n",
            "train loss:0.10114205122286958\n",
            "train loss:0.054575388002973256\n",
            "train loss:0.037525993481744936\n",
            "train loss:0.11552131594144549\n",
            "train loss:0.03311772626376888\n",
            "train loss:0.0821123348913269\n",
            "train loss:0.07684616464712238\n",
            "train loss:0.07163774734308\n",
            "train loss:0.05562103429434945\n",
            "train loss:0.015270084828298487\n",
            "train loss:0.03875463069025544\n",
            "train loss:0.03964485945101333\n",
            "train loss:0.0255223540241033\n",
            "train loss:0.0857705998724436\n",
            "train loss:0.061889578241210115\n",
            "train loss:0.05475006544623253\n",
            "train loss:0.05976503821779529\n",
            "train loss:0.037724573733311846\n",
            "train loss:0.017805837470351193\n",
            "train loss:0.10638270347081549\n",
            "train loss:0.08556162497932888\n",
            "train loss:0.023377143643201218\n",
            "train loss:0.0381062694595901\n",
            "train loss:0.0767993745837134\n",
            "train loss:0.06620968951044141\n",
            "train loss:0.015748421164073437\n",
            "train loss:0.027456685297913234\n",
            "train loss:0.034828099648516846\n",
            "train loss:0.0665704600605712\n",
            "train loss:0.1307525973145407\n",
            "train loss:0.06671307925225846\n",
            "train loss:0.04290580595277769\n",
            "train loss:0.08536338860635953\n",
            "=== epoch:3, train acc:0.978, test acc:0.974 ===\n",
            "train loss:0.037826250124825454\n",
            "train loss:0.013713860236750076\n",
            "train loss:0.04627638362486272\n",
            "train loss:0.047389047093523776\n",
            "train loss:0.10488744761217944\n",
            "train loss:0.11978670666079747\n",
            "train loss:0.06794613651013207\n",
            "train loss:0.023609798316960834\n",
            "train loss:0.10456807593074809\n",
            "train loss:0.04518484453626125\n",
            "train loss:0.027950580410408717\n",
            "train loss:0.11445195959458004\n",
            "train loss:0.023938742501978482\n",
            "train loss:0.06158793123027907\n",
            "train loss:0.07149892663091804\n",
            "train loss:0.20236782389647195\n",
            "train loss:0.07188983602220583\n",
            "train loss:0.042282853449784275\n",
            "train loss:0.05441560187793847\n",
            "train loss:0.06658300308827236\n",
            "train loss:0.07296952606218725\n",
            "train loss:0.045336259729715056\n",
            "train loss:0.05203769449780447\n",
            "train loss:0.11405893764511837\n",
            "train loss:0.04588449348312662\n",
            "train loss:0.014267153483277926\n",
            "train loss:0.04479287408153055\n",
            "train loss:0.1441954582569177\n",
            "train loss:0.22016518826324277\n",
            "train loss:0.07934016423950245\n",
            "train loss:0.12352819888872428\n",
            "train loss:0.044026623628708725\n",
            "train loss:0.032453309672449505\n",
            "train loss:0.05128329772157536\n",
            "train loss:0.08125629746601938\n",
            "train loss:0.05689294892289375\n",
            "train loss:0.07322634622594715\n",
            "train loss:0.0656744998799534\n",
            "train loss:0.08793470496200656\n",
            "train loss:0.07045787029525254\n",
            "train loss:0.07313185231255334\n",
            "train loss:0.0750157731826937\n",
            "train loss:0.14584929755395123\n",
            "train loss:0.030020979828286\n",
            "train loss:0.14808864452095757\n",
            "train loss:0.06899177372895225\n",
            "train loss:0.10259297412207644\n",
            "train loss:0.1758580656137184\n",
            "train loss:0.07678500965170104\n",
            "train loss:0.05703792755057961\n",
            "train loss:0.07096026183146374\n",
            "train loss:0.02668864240753677\n",
            "train loss:0.14396566423662635\n",
            "train loss:0.05146707494107584\n",
            "train loss:0.05865804065590851\n",
            "train loss:0.0734213556185252\n",
            "train loss:0.0742345354359551\n",
            "train loss:0.019454688485837562\n",
            "train loss:0.10021382016357766\n",
            "train loss:0.03220854769000084\n",
            "train loss:0.10388970276101547\n",
            "train loss:0.09011639733074137\n",
            "train loss:0.06884930629038888\n",
            "train loss:0.04025531243803955\n",
            "train loss:0.09929515032500161\n",
            "train loss:0.08119903621223021\n",
            "train loss:0.12145460192979404\n",
            "train loss:0.07607547585110883\n",
            "train loss:0.10521544253962492\n",
            "train loss:0.08010603111798406\n",
            "train loss:0.026045766723926625\n",
            "train loss:0.09007904865859681\n",
            "train loss:0.11789956673719935\n",
            "train loss:0.044760094365417606\n",
            "train loss:0.04918632526507438\n",
            "train loss:0.09352884847022475\n",
            "train loss:0.08350851214082658\n",
            "train loss:0.06347489359435121\n",
            "train loss:0.02438094324883525\n",
            "train loss:0.06653536013137154\n",
            "train loss:0.029012784630554758\n",
            "train loss:0.04866602867156151\n",
            "train loss:0.09816836406556978\n",
            "train loss:0.04624008617256415\n",
            "train loss:0.10182469007280429\n",
            "train loss:0.07995522424149071\n",
            "train loss:0.047736013694820834\n",
            "train loss:0.030598803977591786\n",
            "train loss:0.03579903906107293\n",
            "train loss:0.0879420656563266\n",
            "train loss:0.022065400119144988\n",
            "train loss:0.04270665328786356\n",
            "train loss:0.027270302222842404\n",
            "train loss:0.023652004607532237\n",
            "train loss:0.05835559500986023\n",
            "train loss:0.0938314538122232\n",
            "train loss:0.01657281264808571\n",
            "train loss:0.018780805322814155\n",
            "train loss:0.1388658969002517\n",
            "train loss:0.08395002658088786\n",
            "train loss:0.014735879649444998\n",
            "train loss:0.01929864924761177\n",
            "train loss:0.04015164216654601\n",
            "train loss:0.09562769725534653\n",
            "train loss:0.08608766059360773\n",
            "train loss:0.022657444833605257\n",
            "train loss:0.029672599569882503\n",
            "train loss:0.055915708871980156\n",
            "train loss:0.027570426830534324\n",
            "train loss:0.03709912527958827\n",
            "train loss:0.01812906197289551\n",
            "train loss:0.021936491036194116\n",
            "train loss:0.13501314014095672\n",
            "train loss:0.025476998684076223\n",
            "train loss:0.08908611670770554\n",
            "train loss:0.05174918564287431\n",
            "train loss:0.023465742328883873\n",
            "train loss:0.057319123983680005\n",
            "train loss:0.06339401413760724\n",
            "train loss:0.024267063751780116\n",
            "train loss:0.08627457047638781\n",
            "train loss:0.0623664284091337\n",
            "train loss:0.031782067352695446\n",
            "train loss:0.15415713538223016\n",
            "train loss:0.06680310644746486\n",
            "train loss:0.042505422668404645\n",
            "train loss:0.020053502140140025\n",
            "train loss:0.08274118044627227\n",
            "train loss:0.11098094393014674\n",
            "train loss:0.04266167795179501\n",
            "train loss:0.035381674117620235\n",
            "train loss:0.04175397117600891\n",
            "train loss:0.030661909062653536\n",
            "train loss:0.01755564474267221\n",
            "train loss:0.029676619537025856\n",
            "train loss:0.06373275808531882\n",
            "train loss:0.029152830018948372\n",
            "train loss:0.06721877502897852\n",
            "train loss:0.04978357953280871\n",
            "train loss:0.07636165241919207\n",
            "train loss:0.04109922157566011\n",
            "train loss:0.08530558886850337\n",
            "train loss:0.08513442201901161\n",
            "train loss:0.03328773040998844\n",
            "train loss:0.10827758365008913\n",
            "train loss:0.022685901472969096\n",
            "train loss:0.031259443844490246\n",
            "train loss:0.023243687543585766\n",
            "train loss:0.032771774818278046\n",
            "train loss:0.022728657438453665\n",
            "train loss:0.12448940381728084\n",
            "train loss:0.0323816572353602\n",
            "train loss:0.03135337287668038\n",
            "train loss:0.08250574122324228\n",
            "train loss:0.06406240696743591\n",
            "train loss:0.08643261426058439\n",
            "train loss:0.10368178666548705\n",
            "train loss:0.03248970874039556\n",
            "train loss:0.07565478898289549\n",
            "train loss:0.06741521148782059\n",
            "train loss:0.03497385143002423\n",
            "train loss:0.025209858951381007\n",
            "train loss:0.06775642182571599\n",
            "train loss:0.04988206172441557\n",
            "train loss:0.03776461122761525\n",
            "train loss:0.0618788719795334\n",
            "train loss:0.06290872827081676\n",
            "train loss:0.03200877123861612\n",
            "train loss:0.08895183897097501\n",
            "train loss:0.03736680345995933\n",
            "train loss:0.049015739127561145\n",
            "train loss:0.039360509865945366\n",
            "train loss:0.08030861774983068\n",
            "train loss:0.05665115326650165\n",
            "train loss:0.17957912474357085\n",
            "train loss:0.03625951914109875\n",
            "train loss:0.013682406733148212\n",
            "train loss:0.06844597110199202\n",
            "train loss:0.06039293658523328\n",
            "train loss:0.06760878628429344\n",
            "train loss:0.03155769759306532\n",
            "train loss:0.04488839883036788\n",
            "train loss:0.04284123275148742\n",
            "train loss:0.08499418579105082\n",
            "train loss:0.05837241573327469\n",
            "train loss:0.028414954137987926\n",
            "train loss:0.01660128063503983\n",
            "train loss:0.01993367071820337\n",
            "train loss:0.08165589053219756\n",
            "train loss:0.03943677575889709\n",
            "train loss:0.026000354788273145\n",
            "train loss:0.020560759129858754\n",
            "train loss:0.005664368184642939\n",
            "train loss:0.06385968519805346\n",
            "train loss:0.06900417055905217\n",
            "train loss:0.04841072972333468\n",
            "train loss:0.020319352303797576\n",
            "train loss:0.03945781904132848\n",
            "train loss:0.049451283067228784\n",
            "train loss:0.0262303844303606\n",
            "train loss:0.024401003838281258\n",
            "train loss:0.010262478811672358\n",
            "train loss:0.028969470574406464\n",
            "train loss:0.026972468941535335\n",
            "train loss:0.02718872265259884\n",
            "train loss:0.10843453295604955\n",
            "train loss:0.02264421890608881\n",
            "train loss:0.05711596684395251\n",
            "train loss:0.09758094866399293\n",
            "train loss:0.009831414933007083\n",
            "train loss:0.012989153219204153\n",
            "train loss:0.03708823099036688\n",
            "train loss:0.048480941414922867\n",
            "train loss:0.051989477758407954\n",
            "train loss:0.06008513471553674\n",
            "train loss:0.06871352326378585\n",
            "train loss:0.036999011848361366\n",
            "train loss:0.05593961415229063\n",
            "train loss:0.024234803453008495\n",
            "train loss:0.0439908023198506\n",
            "train loss:0.04252775340070742\n",
            "train loss:0.08150369888720876\n",
            "train loss:0.04724044714400866\n",
            "train loss:0.011707060525700974\n",
            "train loss:0.017431278350285587\n",
            "train loss:0.06815828095264287\n",
            "train loss:0.09054528113157424\n",
            "train loss:0.035198472318270065\n",
            "train loss:0.02226670529398857\n",
            "train loss:0.02227410876442046\n",
            "train loss:0.06152065910873482\n",
            "train loss:0.028096525938537963\n",
            "train loss:0.08448416455186468\n",
            "train loss:0.02015406357823648\n",
            "train loss:0.01826917746205038\n",
            "train loss:0.0871988144229732\n",
            "train loss:0.012190187318646437\n",
            "train loss:0.01894239163168735\n",
            "train loss:0.05768692924522375\n",
            "train loss:0.042456314595631574\n",
            "train loss:0.035670010530325175\n",
            "train loss:0.07807255765331282\n",
            "train loss:0.018005090178212443\n",
            "train loss:0.044883837937144205\n",
            "train loss:0.043898253425368884\n",
            "train loss:0.027171104298156865\n",
            "train loss:0.053829475431734435\n",
            "train loss:0.02877554054368649\n",
            "train loss:0.027887124238615835\n",
            "train loss:0.05172968077834859\n",
            "train loss:0.019006219380395255\n",
            "train loss:0.009967893113841888\n",
            "train loss:0.028319923647824874\n",
            "train loss:0.03496728398048075\n",
            "train loss:0.03362446681586796\n",
            "train loss:0.026899965399981615\n",
            "train loss:0.07989079658656792\n",
            "train loss:0.11606196864973402\n",
            "train loss:0.10731464228535166\n",
            "train loss:0.04834401451387957\n",
            "train loss:0.06917743978627958\n",
            "train loss:0.04058868527263475\n",
            "train loss:0.02351952400248197\n",
            "train loss:0.03273090432447301\n",
            "train loss:0.09770286773839235\n",
            "train loss:0.09275608320932367\n",
            "train loss:0.05479426659545075\n",
            "train loss:0.011763045112371364\n",
            "train loss:0.02154873142295163\n",
            "train loss:0.02714933174217681\n",
            "train loss:0.008670068662816631\n",
            "train loss:0.009049916995286645\n",
            "train loss:0.12303329879825758\n",
            "train loss:0.0640365222825015\n",
            "train loss:0.05804647381041546\n",
            "train loss:0.043637573532979014\n",
            "train loss:0.026968417833188253\n",
            "train loss:0.06403601218183722\n",
            "train loss:0.03122180192786323\n",
            "train loss:0.08762947893243143\n",
            "train loss:0.048588144024782565\n",
            "train loss:0.03767613079931026\n",
            "train loss:0.022748185627515988\n",
            "train loss:0.10147458924578041\n",
            "train loss:0.02503746707670822\n",
            "train loss:0.06406765998217467\n",
            "train loss:0.035327761177515786\n",
            "train loss:0.07996213935257654\n",
            "train loss:0.06210197027717723\n",
            "train loss:0.028444468609337106\n",
            "train loss:0.06599978289143894\n",
            "train loss:0.07919138912353431\n",
            "train loss:0.05265310297206025\n",
            "train loss:0.0543983508660954\n",
            "train loss:0.04053621341629692\n",
            "train loss:0.053414639689655895\n",
            "train loss:0.039458767568069326\n",
            "train loss:0.06998584591603305\n",
            "train loss:0.06907400246266628\n",
            "train loss:0.019195689396766625\n",
            "train loss:0.08384771331014818\n",
            "train loss:0.0697207442327773\n",
            "train loss:0.05634565808170447\n",
            "train loss:0.016981137967892378\n",
            "train loss:0.052651968196138814\n",
            "train loss:0.036898953795353615\n",
            "train loss:0.06395966294035113\n",
            "train loss:0.12088794575959609\n",
            "train loss:0.05956121181292507\n",
            "train loss:0.07912992113489334\n",
            "train loss:0.06497275288241927\n",
            "train loss:0.054547818698967715\n",
            "train loss:0.08716288846035186\n",
            "train loss:0.022765897597331095\n",
            "train loss:0.03607463523643291\n",
            "train loss:0.056786747832544664\n",
            "train loss:0.02855440933695787\n",
            "train loss:0.0529464250524442\n",
            "train loss:0.047717396825287306\n",
            "train loss:0.04685040789974518\n",
            "train loss:0.07820929038189535\n",
            "train loss:0.03513628847348025\n",
            "train loss:0.05889961644594557\n",
            "train loss:0.05910876382524254\n",
            "train loss:0.044259511932301585\n",
            "train loss:0.11013408714207738\n",
            "train loss:0.10261945865541175\n",
            "train loss:0.05181832607662314\n",
            "train loss:0.10796297797162327\n",
            "train loss:0.0487718689942842\n",
            "train loss:0.029818291540677486\n",
            "train loss:0.01978438224008814\n",
            "train loss:0.05737148684429571\n",
            "train loss:0.065398047401525\n",
            "train loss:0.03529505774898211\n",
            "train loss:0.066040017847605\n",
            "train loss:0.04018571969724042\n",
            "train loss:0.02726285879920471\n",
            "train loss:0.024388661096542464\n",
            "train loss:0.046987645407018164\n",
            "train loss:0.05175530668307414\n",
            "train loss:0.03655254366533909\n",
            "train loss:0.08389272535545322\n",
            "train loss:0.052042680552812026\n",
            "train loss:0.13985342236204493\n",
            "train loss:0.02102047062307644\n",
            "train loss:0.025118566896546696\n",
            "train loss:0.019343968775821875\n",
            "train loss:0.08881964325325489\n",
            "train loss:0.03217166930685501\n",
            "train loss:0.017840556654055494\n",
            "train loss:0.010882269515357857\n",
            "train loss:0.033614227493119535\n",
            "train loss:0.0195132753807674\n",
            "train loss:0.014342717104082572\n",
            "train loss:0.045382063723239614\n",
            "train loss:0.04981958038386056\n",
            "train loss:0.04440689299818987\n",
            "train loss:0.05607030141812427\n",
            "train loss:0.04153679228603247\n",
            "train loss:0.09322543740610513\n",
            "train loss:0.09506026928143171\n",
            "train loss:0.02928058375851203\n",
            "train loss:0.032638102258400234\n",
            "train loss:0.056429665397788875\n",
            "train loss:0.010712665580378047\n",
            "train loss:0.08569616962385931\n",
            "train loss:0.06607427619644828\n",
            "train loss:0.02510877223127301\n",
            "train loss:0.009567487415486846\n",
            "train loss:0.13409601636292076\n",
            "train loss:0.03528127649403937\n",
            "train loss:0.07246709894732022\n",
            "train loss:0.020770641891605535\n",
            "train loss:0.034534413001658104\n",
            "train loss:0.03820640405244766\n",
            "train loss:0.135401298488961\n",
            "train loss:0.03369422059379458\n",
            "train loss:0.031674382664142665\n",
            "train loss:0.05647712178036011\n",
            "train loss:0.04083198431753546\n",
            "train loss:0.027368765783503005\n",
            "train loss:0.014382383284012782\n",
            "train loss:0.04029451465637167\n",
            "train loss:0.1193997738237187\n",
            "train loss:0.04372336814907226\n",
            "train loss:0.03123805849957906\n",
            "train loss:0.0709928301882971\n",
            "train loss:0.1451362250845655\n",
            "train loss:0.05213023098261773\n",
            "train loss:0.015198365691455364\n",
            "train loss:0.03802572959237932\n",
            "train loss:0.02107999558558993\n",
            "train loss:0.07074659626030888\n",
            "train loss:0.017997975978121108\n",
            "train loss:0.05161594775186435\n",
            "train loss:0.04160775841055731\n",
            "train loss:0.10304779438751539\n",
            "train loss:0.12446143428533825\n",
            "train loss:0.011219018389260836\n",
            "train loss:0.034960073995300434\n",
            "train loss:0.13432154768501162\n",
            "train loss:0.07538155180598188\n",
            "train loss:0.030117448758514458\n",
            "train loss:0.031534984212066595\n",
            "train loss:0.03679781923975522\n",
            "train loss:0.010962086133380343\n",
            "train loss:0.03257457249090223\n",
            "train loss:0.06031662487842209\n",
            "train loss:0.029387518134643312\n",
            "train loss:0.03309713727283057\n",
            "train loss:0.12226620251971473\n",
            "train loss:0.07925111560969056\n",
            "train loss:0.03650484436995864\n",
            "train loss:0.017261194838362835\n",
            "train loss:0.07104910219069352\n",
            "train loss:0.05153882241701714\n",
            "train loss:0.057186272251418584\n",
            "train loss:0.033310021186921675\n",
            "train loss:0.08246504414999395\n",
            "train loss:0.13666878655420148\n",
            "train loss:0.010028104137225175\n",
            "train loss:0.021642698238239718\n",
            "train loss:0.07844930167398631\n",
            "train loss:0.012078078098604625\n",
            "train loss:0.049303775718386546\n",
            "train loss:0.041533141819490346\n",
            "train loss:0.03780575995985042\n",
            "train loss:0.06685529411586345\n",
            "train loss:0.06620903050834982\n",
            "train loss:0.060286726483883585\n",
            "train loss:0.135226886910524\n",
            "train loss:0.10770357389016233\n",
            "train loss:0.01748904832483625\n",
            "train loss:0.07799311137494311\n",
            "train loss:0.06382075428570726\n",
            "train loss:0.11385576635039708\n",
            "train loss:0.1144940815356618\n",
            "train loss:0.031783927594875146\n",
            "train loss:0.060279161795426445\n",
            "train loss:0.03449477367997831\n",
            "train loss:0.03013728767421466\n",
            "train loss:0.04594422545980166\n",
            "train loss:0.019708075567828304\n",
            "train loss:0.03432858553221176\n",
            "train loss:0.030541309609471264\n",
            "train loss:0.040024631164649715\n",
            "train loss:0.14057708251815157\n",
            "train loss:0.06226550608760381\n",
            "train loss:0.04876500968357919\n",
            "train loss:0.01888782056207735\n",
            "train loss:0.05000130256176535\n",
            "train loss:0.03101898799565506\n",
            "train loss:0.03071310140988499\n",
            "train loss:0.04341550950195374\n",
            "train loss:0.0589508127727018\n",
            "train loss:0.02648151060573783\n",
            "train loss:0.09657091643269736\n",
            "train loss:0.02137686559990712\n",
            "train loss:0.017965645932521472\n",
            "train loss:0.04961023432860886\n",
            "train loss:0.07568247172854721\n",
            "train loss:0.032812852353057836\n",
            "train loss:0.04301113373641896\n",
            "train loss:0.015609228064432091\n",
            "train loss:0.06120947051931317\n",
            "train loss:0.012509085636449195\n",
            "train loss:0.05274489007619368\n",
            "train loss:0.05784999171517568\n",
            "train loss:0.041506158578689265\n",
            "train loss:0.015104726140303728\n",
            "train loss:0.012844714209782022\n",
            "train loss:0.012884473290869392\n",
            "train loss:0.01711395191594778\n",
            "train loss:0.05662648355588877\n",
            "train loss:0.06650886580414367\n",
            "train loss:0.04631663405642313\n",
            "train loss:0.08863319092650927\n",
            "train loss:0.08929723594458679\n",
            "train loss:0.032197095035699896\n",
            "train loss:0.05784671329135356\n",
            "train loss:0.06517161426090469\n",
            "train loss:0.019470781383721925\n",
            "train loss:0.05525316889878761\n",
            "train loss:0.04508198876469522\n",
            "train loss:0.06578746630394297\n",
            "train loss:0.020073865118814663\n",
            "train loss:0.08136544905145233\n",
            "train loss:0.081000543825963\n",
            "train loss:0.060617986562954196\n",
            "train loss:0.08589718939428666\n",
            "train loss:0.020010314736782494\n",
            "train loss:0.03448112162052914\n",
            "train loss:0.04434342241683419\n",
            "train loss:0.07266616829802523\n",
            "train loss:0.0822327795485277\n",
            "train loss:0.016118331230130963\n",
            "train loss:0.08734408132042812\n",
            "train loss:0.03379837454863001\n",
            "train loss:0.04831550877539833\n",
            "train loss:0.06385898605116237\n",
            "train loss:0.09540274830605618\n",
            "train loss:0.04142345674979286\n",
            "train loss:0.011820373549715308\n",
            "train loss:0.00879256726240563\n",
            "train loss:0.027136279864859288\n",
            "train loss:0.0650908275551328\n",
            "train loss:0.016163415224393286\n",
            "train loss:0.0569051088918797\n",
            "train loss:0.12257934435110433\n",
            "train loss:0.01331054634174169\n",
            "train loss:0.06310892997310863\n",
            "train loss:0.10079097986279173\n",
            "train loss:0.04038418786042389\n",
            "train loss:0.03183062060934235\n",
            "train loss:0.018167034439708858\n",
            "train loss:0.10159228984094434\n",
            "train loss:0.016681627870840935\n",
            "train loss:0.007815968272087852\n",
            "train loss:0.024887235886538378\n",
            "train loss:0.07771270584933028\n",
            "train loss:0.024454051951397764\n",
            "train loss:0.06025182547938036\n",
            "train loss:0.08529210789696913\n",
            "train loss:0.052566985062018484\n",
            "train loss:0.06445451085863742\n",
            "train loss:0.04043715010830293\n",
            "train loss:0.06562774638046999\n",
            "train loss:0.015274360759245577\n",
            "train loss:0.07273783332826843\n",
            "train loss:0.026989819034288542\n",
            "train loss:0.0826572392926235\n",
            "train loss:0.03366286927915103\n",
            "train loss:0.027115590632894277\n",
            "train loss:0.09720948263816258\n",
            "train loss:0.04278648768768709\n",
            "train loss:0.017497869010770706\n",
            "train loss:0.012540242737805216\n",
            "train loss:0.02849090351099071\n",
            "train loss:0.1380353405296059\n",
            "train loss:0.02450266564262726\n",
            "train loss:0.045665520106353635\n",
            "train loss:0.0280289353615877\n",
            "train loss:0.04463062606824695\n",
            "train loss:0.04695423611547225\n",
            "train loss:0.07720985431983511\n",
            "train loss:0.1565808656624064\n",
            "train loss:0.02723707186695283\n",
            "train loss:0.015321944816468466\n",
            "train loss:0.02089482774322945\n",
            "train loss:0.04334801821794134\n",
            "train loss:0.04981280562730663\n",
            "train loss:0.07069986258323743\n",
            "train loss:0.03587629899890325\n",
            "train loss:0.06833252578044852\n",
            "train loss:0.03127695644428897\n",
            "train loss:0.03810197760847173\n",
            "train loss:0.03902278044523998\n",
            "train loss:0.03552260773786345\n",
            "train loss:0.07084916467726439\n",
            "train loss:0.012190250409758203\n",
            "train loss:0.07742415517326401\n",
            "train loss:0.03586641796683893\n",
            "train loss:0.0941908948392461\n",
            "train loss:0.0369922442986348\n",
            "train loss:0.025648430479791456\n",
            "train loss:0.0610280218261818\n",
            "train loss:0.04674794483696469\n",
            "train loss:0.024790669667851928\n",
            "train loss:0.02748750572053498\n",
            "train loss:0.10382536524040051\n",
            "train loss:0.06693583399900263\n",
            "train loss:0.012602574305081998\n",
            "train loss:0.01501884293641254\n",
            "train loss:0.01743881444080113\n",
            "train loss:0.05982066509598847\n",
            "train loss:0.012125505368021254\n",
            "train loss:0.036763674769059215\n",
            "train loss:0.031091556599889145\n",
            "train loss:0.08838742172377043\n",
            "train loss:0.012011039065678623\n",
            "train loss:0.04159201485673503\n",
            "train loss:0.014244883599240669\n",
            "train loss:0.024714700764629935\n",
            "train loss:0.07314973213347806\n",
            "train loss:0.07545418018805876\n",
            "train loss:0.00536865211771654\n",
            "train loss:0.09783403222234956\n",
            "train loss:0.030227061905759207\n",
            "train loss:0.030789590273279432\n",
            "train loss:0.07825522963919507\n",
            "train loss:0.01335422626099821\n",
            "train loss:0.08540202923570636\n",
            "train loss:0.018038058237474737\n",
            "train loss:0.06149726881893216\n",
            "train loss:0.008120406969123967\n",
            "train loss:0.06328578194532226\n",
            "train loss:0.010624627657400559\n",
            "train loss:0.020703604510935437\n",
            "=============== Final Test Accuracy ===============\n",
            "test acc:0.9837\n",
            "Saved Network Parameters!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFcCAYAAADh1zYWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRdAb4GUE7YYAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f4d3c361780>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "IJPEaOU_mJPl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "fe62ce38-4f78-46d7-c495-fb45c45531a7"
      },
      "cell_type": "code",
      "source": [
        "!ls # params.pkl"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "adc.json      multilayernetextend.py  t10k-images-idx3-ubyte.gz\n",
            "functions.py  multilayernet.py\t      t10k-labels-idx1-ubyte.gz\n",
            "gradient.py   optimizer.py\t      trainer.py\n",
            "layers.py     params.pkl\t      train-images-idx3-ubyte.gz\n",
            "mnist.pkl     __pycache__\t      train-labels-idx1-ubyte.gz\n",
            "mnist.py      sample_data\t      util.py\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}